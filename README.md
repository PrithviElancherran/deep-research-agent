# Deep Research Agent (LangGraph + LangChain)

A simple, configurable **deep research agent** built with **LangGraph** and **LangChain**.

- Takes a **user query**
- Performs **web search (Tavily)** to gather real-world context
- Returns a **structured report** grounded in those search results (with citations)

---

## Features

- Uses **LangGraph** with a `messages` field in state
- Accepts a **query** and returns a **final report** as an assistant message
- Uses **Tavily** web search via the modern `langchain-tavily` integration
- Report is **grounded in web context** and includes a **Sources** section
- Agent is **configurable** (model, temperature, number of searches, etc.)
- Includes **simple evals** to check basic report quality

---

## Tech Stack

- Python 3.11+
- [LangChain](https://python.langchain.com/)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [OpenAI via `langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai)
- [Tavily Search via `langchain-tavily`](https://github.com/tavily-ai/langchain-tavily)
- `python-dotenv` for loading environment variables

---

## Setup

### 1. Clone the repo

```bash
git clone https://github.com/PrithviElancherran/deep-research-agent.git
cd deep-research-agent
```

### 2. Create and activate a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate  # macOS / Linux
# .venv\Scripts\Activate   # Windows (PowerShell)
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file in the project root with the following content:

```env
# Required API keys
OPENAI_API_KEY=your_openai_key_here
TAVILY_API_KEY=your_tavily_key_here

# Optional configuration
OPENAI_MODEL=gpt-4.1-mini                 # OpenAI model to use
OPENAI_TEMPERATURE=0.2                    # Response creativity (0 = more factual)

MAX_SEARCH_RESULTS=5                      # Number of web results per Tavily search
NUM_SEARCH_ROUNDS=2                       # Number of search rounds (depth)

SYSTEM_PROMPT=You are a careful deep research assistant. You MUST ground your answer in the provided search results. If something is not supported by the sources, say so clearly. Write a structured report with headings: Overview, Key Findings, Details, and Sources.
```

> **Note:**  
> `.env` is included in `.gitignore`, so your API keys will **not** be committed to GitHub.

---

## Running the Agent (CLI)

Run the agent (with any of the queries given below or feel free to use your own):

```bash
python -m src.cli "What are the key challenges in scaling quantum computers?"

# Optional Queries
python -m src.cli "Compare the environmental impact of electric cars vs. gasoline cars."
python -m src.cli "Summarize the current state of regulation around AI safety in the EU."
```

---

## Sample Report

If you’d like to see an example output without running the code, check:

- [`example_report.md`](./example_report.md)

This file was generated by the agent for the query:

> "Compare the environmental impact of electric cars vs. gasoline cars."

---

## Configuration

| Variable | Description |
|---------|-------------|
| `OPENAI_MODEL` | Chat model (e.g., gpt-4.1-mini) |
| `OPENAI_TEMPERATURE` | Creativity level |
| `MAX_SEARCH_RESULTS` | Max results per search |
| `NUM_SEARCH_ROUNDS` | Number of search rounds |
| `SYSTEM_PROMPT` | Instructions for writing the report |

---

## Architecture

### State

```python
class ResearchState(TypedDict):
    messages: List[BaseMessage]
```

### Nodes

- **search_node** → Tavily search  
- **report_node** → OpenAI structured report  

### Graph Flow

```
entry → search → report → END
```

---

## Evals

Run:

```bash
python -m tests.test_evals
```

---

## Performance Improvements & Future Work

Although this project focuses on building a minimal deep-research agent, there are several clear opportunities for enhancement. Below are improvements I’ve already implemented, followed by higher-impact ideas that could evolve this into a production-ready system. I include these to demonstrate my thought process, trade-off awareness, and ideas for scaling.

### 1. Improve Search Result Quality (Implemented: Domain Preference)

To increase trust in the generated reports, I added a simple heuristic that prioritizes high-quality domains such as:

- `.gov`
- `.edu`
- `.org`
- Well-established scientific sources like `arxiv.org`, `nature.com`

**Improvement rationale:**  
Search APIs often return mixed-quality content. A heuristic filter provides a lightweight way to increase report reliability without additional API calls.

**Future upgrade:**  
Replace heuristic sorting with a credibility-scoring pipeline using:

- Domain authority metrics  
- Citation frequency  
- Last updated timestamps  
- LLM-based “credibility classifier” on extracted text  

This would dramatically improve factual grounding.

---

### 2. Reduce Hallucinations with Evidence Enforcement

Right now the report is grounded in search results through prompting. To further reduce hallucinations:

- Enforce **source-linked claims** (each major claim should be traceable to one or more sources)  
- Introduce a **fact-checking node** that validates claims against extracted text  
- Use **LLM-generated JSON evidence maps** (hidden from the user)  
- Refuse or flag unverifiable claims during report generation  

This would make the system more robust and enterprise-ready.

---

### 3. Add a Retrieval Cache

Web search is relatively slow and costly. Caching would improve UX and reduce API usage.

A simple cache (SQLite, Redis, or even JSON) keyed by:

(query + search_parameters)

would enable:

- Faster repeat queries  
- More offline usage  
- Lower API spend  

Even a small cache can significantly reduce latency in iterative research workflows.

---

### 4. Parallelize Search & Summarization (Leverage LangGraph)

LangGraph supports parallel node execution.  
A more advanced architecture would split tasks into concurrent branches, for example:

- Web search  
- Snippet extraction  
- Chunk summarization  
- Source scoring / filtering  
- Report drafting  

This would improve throughput and demonstrate deeper use of LangGraph’s strengths.

---

### 5. Pluggable Report Format System

Right now the output is Markdown. Future work could add:

- PDF generation  
- HTML export  
- JSON structured summaries  
- Notebook-friendly outputs (for data scientists / researchers)  
- Streamed token output for long reports  

This would make the tool much more flexible across different user workflows.

---

### 6. Automated Evaluation Suite

The project includes simple evals, but future extensions could add:

- Style compliance tests (structure, tone, section presence)  
- Hallucination detection via reference answers or gold labels  
- LLM-as-a-judge grading for coherence and groundedness  
- Citation verification (check that cited URLs appear in retrieved search results)  
- Basic safety checks on generated content  

This would strengthen confidence in the system’s reliability and make it easier to iterate safely.

---

### 7. Additional Optional Enhancements

A few more directions worth exploring:

- Multi-search fusion: combine Tavily with other providers (e.g., SerpAPI, Exa)  
- Long-context models: use long-context models to handle larger sets of sources  
- User-configurable search depth: expose NUM_SEARCH_ROUNDS and MAX_SEARCH_RESULTS in a simple UI  
- Query planning: add a planning node that decomposes complex queries into sub-questions before searching  
- Relevance-based pruning: remove noisy or redundant search results before report generation  

---
